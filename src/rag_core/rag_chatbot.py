import os
import json
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

from langchain_community.vectorstores import FAISS

try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.llms import Ollama
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser


@dataclass
class RAGConfig:
    """Cáº¥u hÃ¬nh cho RAG Bot"""
    vector_db_path: str = "./vector_db"
    embedding_model: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    llm_provider: str = "openai"  # "openai", "ollama"
    llm_model: str = "gpt-3.5-turbo"  # hoáº·c "llama2", "mistral" cho Ollama
    openai_api_key: Optional[str] = None
    temperature: float = 0.1
    max_tokens: int = 1000
    retrieval_k: int = 5  # Sá»‘ documents Ä‘Æ°á»£c retrieve
    score_threshold: float = 0.3  # Giáº£m ngÆ°á»¡ng Ä‘iá»ƒm Ä‘á»ƒ dá»… tÃ¬m tháº¥y tÃ i liá»‡u hÆ¡n


class VietnameseLegalRAGBot:
    """RAG Bot chuyÃªn vá» tÃ i liá»‡u phÃ¡p lÃ½ Viá»‡t Nam"""

    def __init__(self, config: RAGConfig):
        self.config = config
        self.vector_store = None
        self.llm = None
        self.retriever = None
        self.rag_chain = None

    def initialize_embeddings(self, use_openai: bool = False):
        """Khá»Ÿi táº¡o embedding model"""
        if use_openai and self.config.openai_api_key:
            return OpenAIEmbeddings(
                openai_api_key=self.config.openai_api_key,
                model="text-embedding-ada-002"
            )
        else:
            return HuggingFaceEmbeddings(
                model_name=self.config.embedding_model,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )

    def load_vector_store(self):
        """Load vector database"""
        if not os.path.exists(self.config.vector_db_path):
            raise FileNotFoundError(f"Vector database not found at {self.config.vector_db_path}")

        embeddings = self.initialize_embeddings()
        self.vector_store = FAISS.load_local(
            self.config.vector_db_path,
            embeddings,
            allow_dangerous_deserialization=True
        )

        # Táº¡o retriever vá»›i nhiá»u options Ä‘á»ƒ Ä‘áº£m báº£o tÃ¬m Ä‘Æ°á»£c tÃ i liá»‡u
        if self.config.score_threshold > 0:
            self.retriever = self.vector_store.as_retriever(
                search_type="similarity_score_threshold",
                search_kwargs={
                    "k": self.config.retrieval_k,
                    "score_threshold": self.config.score_threshold
                }
            )
        else:
            # Náº¿u khÃ´ng dÃ¹ng threshold, chá»‰ láº¥y k documents tÆ°Æ¡ng tá»± nháº¥t
            self.retriever = self.vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": self.config.retrieval_k}
            )

        print(f"âœ… Loaded vector database from {self.config.vector_db_path}")
        print(f"ğŸ“Š Vector store contains {self.vector_store.index.ntotal} documents")

    def initialize_llm(self):
        """Khá»Ÿi táº¡o Large Language Model"""
        if self.config.llm_provider == "openai":
            if not self.config.openai_api_key:
                print("âš ï¸ Cáº£nh bÃ¡o: KhÃ´ng cÃ³ OpenAI API key, chuyá»ƒn sang sá»­ dá»¥ng Ollama")
                self.config.llm_provider = "ollama"
                self.config.llm_model = "llama2"
            else:
                try:
                    self.llm = ChatOpenAI(
                        api_key=self.config.openai_api_key,
                        model=self.config.llm_model,
                        temperature=self.config.temperature,
                        max_tokens=self.config.max_tokens
                    )
                    # Test the connection
                    test_response = self.llm.invoke("Hello")
                    print(f"âœ… Initialized {self.config.llm_provider} LLM: {self.config.llm_model}")
                    return
                except Exception as e:
                    print(f"âŒ Lá»—i káº¿t ná»‘i OpenAI: {e}")
                    print("ğŸ”„ Chuyá»ƒn sang sá»­ dá»¥ng Ollama...")
                    self.config.llm_provider = "ollama"
                    self.config.llm_model = "llama2"

        if self.config.llm_provider == "ollama":
            try:
                self.llm = Ollama(
                    model=self.config.llm_model,
                    temperature=self.config.temperature
                )
                print(f"âœ… Initialized {self.config.llm_provider} LLM: {self.config.llm_model}")
                print("ğŸ’¡ LÆ°u Ã½: Äáº£m báº£o Ollama Ä‘ang cháº¡y vÃ  model Ä‘Ã£ Ä‘Æ°á»£c táº£i")
            except Exception as e:
                print(f"âŒ Lá»—i khá»Ÿi táº¡o Ollama: {e}")
                print("ğŸ’¡ CÃ i Ä‘áº·t vÃ  cháº¡y Ollama: https://ollama.ai/")
                raise e
        else:
            raise ValueError(f"Unsupported LLM provider: {self.config.llm_provider}")

    def create_prompt_template(self) -> PromptTemplate:
        """Táº¡o prompt template cho RAG"""

        template = """Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn vá» phÃ¡p luáº­t Viá»‡t Nam. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¢u há»i dá»±a trÃªn cÃ¡c tÃ i liá»‡u phÃ¡p lÃ½ Ä‘Æ°á»£c cung cáº¥p.

NGUYÃŠN Táº®C TRá»ŠNH Báº¢Y:
1. Chá»‰ tráº£ lá»i dá»±a trÃªn thÃ´ng tin trong cÃ¡c tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p
2. Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan, hÃ£y nÃ³i rÃµ "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u"
3. TrÃ­ch dáº«n rÃµ rÃ ng Ä‘iá»u luáº­t, nghá»‹ Ä‘á»‹nh liÃªn quan
4. Giáº£i thÃ­ch báº±ng ngÃ´n ngá»¯ dá»… hiá»ƒu, trÃ¡nh thuáº­t ngá»¯ phá»©c táº¡p
5. Náº¿u cÃ³ nhiá»u quy Ä‘á»‹nh liÃªn quan, hÃ£y liá»‡t kÃª Ä‘áº§y Ä‘á»§
6. LuÃ´n chá»‰ rÃµ nguá»“n gá»‘c thÃ´ng tin (tÃªn vÄƒn báº£n, Ä‘iá»u, khoáº£n)

NGá»® Cáº¢NH TÃ€I LIá»†U:
{context}

CÃ‚U Há»I: {question}

HÆ¯á»šNG DáºªN TRáº¢ Lá»œI:
- Báº¯t Ä‘áº§u báº±ng cÃ¢u tráº£ lá»i trá»±c tiáº¿p vÃ  rÃµ rÃ ng
- TrÃ­ch dáº«n cá»¥ thá»ƒ Ä‘iá»u luáº­t hoáº·c quy Ä‘á»‹nh liÃªn quan
- Giáº£i thÃ­ch Ã½ nghÄ©a vÃ  cÃ¡ch Ã¡p dá»¥ng
- Náº¿u cÃ³ cÃ¡c trÆ°á»ng há»£p ngoáº¡i lá»‡, hÃ£y Ä‘á» cáº­p
- Káº¿t thÃºc vá»›i lá»i khuyÃªn thá»±c táº¿ (náº¿u phÃ¹ há»£p)

TRáº¢ Lá»œI:"""

        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )

    def create_detailed_context(self, docs: List[Document]) -> str:
        """Táº¡o context chi tiáº¿t tá»« cÃ¡c documents Ä‘Æ°á»£c retrieve"""
        if not docs:
            return "KhÃ´ng cÃ³ tÃ i liá»‡u liÃªn quan Ä‘Æ°á»£c tÃ¬m tháº¥y."

        context_parts = []

        for i, doc in enumerate(docs, 1):
            metadata = doc.metadata

            # Táº¡o header cho má»—i document
            header = f"--- TÃ€I LIá»†U {i} ---"

            # ThÃ´ng tin metadata
            source_info = []
            if metadata.get('source_file'):
                source_info.append(f"Nguá»“n: {metadata['source_file']}")
            if metadata.get('chapter'):
                source_info.append(f"ChÆ°Æ¡ng: {metadata['chapter']}")
            if metadata.get('chapter_title'):
                source_info.append(f"TiÃªu Ä‘á» chÆ°Æ¡ng: {metadata['chapter_title']}")
            if metadata.get('article_title'):
                source_info.append(f"Äiá»u: {metadata['article_title']}")

            source_line = " | ".join(source_info) if source_info else "Nguá»“n: KhÃ´ng xÃ¡c Ä‘á»‹nh"

            # Ná»™i dung
            content = doc.page_content.strip()

            # Káº¿t há»£p táº¥t cáº£
            doc_text = f"{header}\n{source_line}\n\nNá»˜I DUNG:\n{content}\n"
            context_parts.append(doc_text)

        return "\n".join(context_parts)

    def setup_rag_chain(self):
        """Thiáº¿t láº­p RAG chain hoÃ n chá»‰nh"""
        if not self.vector_store or not self.llm:
            raise ValueError("Vector store and LLM must be initialized first")

        # Táº¡o prompt template
        prompt = self.create_prompt_template()

        # Táº¡o chain vá»›i custom context formatting
        def format_docs(docs):
            return self.create_detailed_context(docs)

        self.rag_chain = (
                {"context": self.retriever | format_docs, "question": RunnablePassthrough()}
                | prompt
                | self.llm
                | StrOutputParser()
        )

        print("âœ… RAG chain setup completed")

    def initialize(self):
        """Khá»Ÿi táº¡o toÃ n bá»™ há»‡ thá»‘ng RAG"""
        print("ğŸš€ Initializing Vietnamese Legal RAG Bot...")

        self.load_vector_store()
        self.initialize_llm()
        self.setup_rag_chain()

        print("âœ… RAG Bot initialized successfully!")

    def test_retrieval(self, question: str) -> List[Document]:
        """Test retrieval function Ä‘á»ƒ debug"""
        if not self.vector_store:
            raise ValueError("Vector store not initialized")

        print(f"ğŸ” Testing retrieval for: {question}")

        # Thá»­ nhiá»u cÃ¡ch tÃ¬m kiáº¿m
        docs_similarity = self.vector_store.similarity_search(question, k=self.config.retrieval_k)
        print(f"ğŸ“„ Similarity search found: {len(docs_similarity)} documents")

        if len(docs_similarity) > 0:
            print("âœ… Found documents with similarity search")
            return docs_similarity

        # Thá»­ vá»›i tá»« khÃ³a Ä‘Æ¡n giáº£n hÆ¡n
        simple_keywords = question.split()[:3]  # Láº¥y 3 tá»« Ä‘áº§u tiÃªn
        simple_query = " ".join(simple_keywords)
        docs_simple = self.vector_store.similarity_search(simple_query, k=self.config.retrieval_k)
        print(f"ğŸ“„ Simple search found: {len(docs_simple)} documents")

        return docs_simple

    def ask(self, question: str, return_sources: bool = True) -> Dict[str, Any]:
        """
        Äáº·t cÃ¢u há»i cho RAG bot

        Args:
            question: CÃ¢u há»i
            return_sources: CÃ³ tráº£ vá» sources khÃ´ng

        Returns:
            Dict chá»©a answer vÃ  sources (náº¿u cÃ³)
        """
        if not self.rag_chain:
            raise ValueError("RAG chain not initialized. Call initialize() first.")

        start_time = datetime.now()

        try:
            # Test retrieval trÆ°á»›c
            relevant_docs = self.test_retrieval(question)

            if not relevant_docs:
                return {
                    "question": question,
                    "answer": "TÃ´i khÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a báº¡n trong cÆ¡ sá»Ÿ dá»¯ liá»‡u. Vui lÃ²ng thá»­ láº¡i vá»›i cÃ¢u há»i khÃ¡c hoáº·c kiá»ƒm tra láº¡i tá»« khÃ³a.",
                    "processing_time": 0,
                    "num_sources": 0,
                    "sources": [] if return_sources else None
                }

            # Generate answer
            answer = self.rag_chain.invoke(question)

            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            result = {
                "question": question,
                "answer": answer,
                "processing_time": processing_time,
                "num_sources": len(relevant_docs)
            }

            if return_sources:
                sources = []
                for doc in relevant_docs:
                    sources.append({
                        "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                        "metadata": doc.metadata
                    })
                result["sources"] = sources

            return result

        except Exception as e:
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()

            return {
                "question": question,
                "answer": f"ÄÃ£ xáº£y ra lá»—i khi xá»­ lÃ½ cÃ¢u há»i: {str(e)}",
                "processing_time": processing_time,
                "num_sources": 0,
                "sources": [] if return_sources else None
            }

    def batch_ask(self, questions: List[str]) -> List[Dict[str, Any]]:
        """Xá»­ lÃ½ nhiá»u cÃ¢u há»i cÃ¹ng lÃºc"""
        results = []
        for question in questions:
            result = self.ask(question)
            results.append(result)
        return results

    def get_related_documents(self, query: str, k: int = 5) -> List[Document]:
        """Láº¥y documents liÃªn quan mÃ  khÃ´ng generate answer"""
        if not self.vector_store:
            raise ValueError("Vector store not initialized")

        return self.vector_store.similarity_search(query, k=k)

    def chat_interface(self):
        """Giao diá»‡n chat Ä‘Æ¡n giáº£n"""
        print("\n" + "=" * 60)
        print("ğŸ¤– Vietnamese Legal RAG Bot")
        print("GÃµ 'quit', 'exit' hoáº·c 'thoÃ¡t' Ä‘á»ƒ káº¿t thÃºc")
        print("GÃµ 'help' Ä‘á»ƒ xem hÆ°á»›ng dáº«n")
        print("GÃµ 'test' Ä‘á»ƒ kiá»ƒm tra há»‡ thá»‘ng")
        print("=" * 60 + "\n")

        while True:
            try:
                question = input("â“ CÃ¢u há»i cá»§a báº¡n: ").strip()

                if question.lower() in ['quit', 'exit', 'thoÃ¡t', 'q']:
                    print("ğŸ‘‹ Táº¡m biá»‡t!")
                    break

                if question.lower() == 'help':
                    self._show_help()
                    continue

                if question.lower() == 'test':
                    self._run_test()
                    continue

                if not question:
                    print("âš ï¸ Vui lÃ²ng nháº­p cÃ¢u há»i")
                    continue

                print("\nğŸ¤” Äang tÃ¬m kiáº¿m vÃ  phÃ¢n tÃ­ch...")

                result = self.ask(question)

                print(f"\nğŸ’¡ **Tráº£ lá»i:**\n{result['answer']}")
                print(
                    f"\nğŸ“Š **Thá»‘ng kÃª:** {result['num_sources']} tÃ i liá»‡u liÃªn quan | Thá»i gian xá»­ lÃ½: {result['processing_time']:.2f}s")

                # Hiá»ƒn thá»‹ sources náº¿u cÃ³
                if result.get('sources') and len(result['sources']) > 0:
                    show_sources = input("\nğŸ“š Báº¡n cÃ³ muá»‘n xem cÃ¡c tÃ i liá»‡u tham kháº£o? (y/n): ").lower().startswith('y')
                    if show_sources:
                        self._display_sources(result['sources'])

                print("\n" + "-" * 60 + "\n")

            except KeyboardInterrupt:
                print("\nğŸ‘‹ Táº¡m biá»‡t!")
                break
            except Exception as e:
                print(f"âŒ Lá»—i: {e}")

    def _run_test(self):
        """Cháº¡y test há»‡ thá»‘ng"""
        print("\nğŸ§ª Äang cháº¡y test há»‡ thá»‘ng...")

        # Test 1: Kiá»ƒm tra vector store
        if self.vector_store:
            total_docs = self.vector_store.index.ntotal
            print(f"âœ… Vector store: {total_docs} documents")
        else:
            print("âŒ Vector store: ChÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o")
            return

        # Test 2: Kiá»ƒm tra LLM
        if self.llm:
            print(f"âœ… LLM: {self.config.llm_provider} - {self.config.llm_model}")
        else:
            print("âŒ LLM: ChÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o")
            return

        # Test 3: Thá»­ tÃ¬m kiáº¿m
        test_queries = ["luáº­t", "nghá»‹ Ä‘á»‹nh", "quy Ä‘á»‹nh", "pháº¡t"]
        for query in test_queries:
            docs = self.vector_store.similarity_search(query, k=2)
            print(f"ğŸ” '{query}': {len(docs)} documents found")
            if len(docs) > 0:
                break
        else:
            print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u nÃ o vá»›i cÃ¡c tá»« khÃ³a cÆ¡ báº£n")

        print("âœ… Test hoÃ n thÃ nh!\n")

    def _show_help(self):
        """Hiá»ƒn thá»‹ hÆ°á»›ng dáº«n sá»­ dá»¥ng"""
        help_text = """
ğŸ“– HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG:

ğŸ” CÃ¡c loáº¡i cÃ¢u há»i báº¡n cÃ³ thá»ƒ Ä‘áº·t:
   â€¢ Há»i vá» quy Ä‘á»‹nh cá»¥ thá»ƒ: "Quy Ä‘á»‹nh vá» thuáº¿ thu nháº­p cÃ¡ nhÃ¢n lÃ  gÃ¬?"
   â€¢ Há»i vá» thá»§ tá»¥c: "Thá»§ tá»¥c Ä‘Äƒng kÃ½ kinh doanh nhÆ° tháº¿ nÃ o?"
   â€¢ Há»i vá» má»©c pháº¡t: "Má»©c pháº¡t vi pháº¡m giao thÃ´ng lÃ  bao nhiÃªu?"
   â€¢ Há»i vá» Ä‘iá»u kiá»‡n: "Äiá»u kiá»‡n Ä‘á»ƒ Ä‘Æ°á»£c miá»…n thuáº¿?"

ğŸ’¡ Máº¹o Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i tá»‘t nháº¥t:
   â€¢ Äáº·t cÃ¢u há»i cá»¥ thá»ƒ vÃ  rÃµ rÃ ng
   â€¢ Sá»­ dá»¥ng tá»« khÃ³a chÃ­nh xÃ¡c
   â€¢ CÃ³ thá»ƒ há»i theo tÃªn luáº­t/nghá»‹ Ä‘á»‹nh cá»¥ thá»ƒ

âš ï¸ LÆ°y Ã½:
   â€¢ Bot chá»‰ tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c táº£i
   â€¢ ThÃ´ng tin chá»‰ mang tÃ­nh tham kháº£o
   â€¢ NÃªn tham kháº£o Ã½ kiáº¿n chuyÃªn gia phÃ¡p lÃ½ cho cÃ¡c váº¥n Ä‘á» phá»©c táº¡p

ğŸ”§ Lá»‡nh Ä‘áº·c biá»‡t:
   â€¢ 'test' - Kiá»ƒm tra há»‡ thá»‘ng
   â€¢ 'help' - Hiá»ƒn thá»‹ hÆ°á»›ng dáº«n
   â€¢ 'quit' - ThoÃ¡t chÆ°Æ¡ng trÃ¬nh
        """
        print(help_text)

    def _display_sources(self, sources: List[Dict]):
        """Hiá»ƒn thá»‹ sources má»™t cÃ¡ch Ä‘áº¹p máº¯t"""
        print("\nğŸ“š **TÃ€I LIá»†U THAM KHáº¢O:**")
        for i, source in enumerate(sources, 1):
            metadata = source['metadata']
            print(f"\n{i}. **{metadata.get('article_title', 'N/A')}**")
            print(f"   ğŸ“„ Nguá»“n: {metadata.get('source_file', 'N/A')}")
            if metadata.get('chapter'):
                print(f"   ğŸ“‘ {metadata['chapter']} - {metadata.get('chapter_title', '')}")
            print(f"   ğŸ“ Ná»™i dung: {source['content']}")


def main():
    """HÃ m main Ä‘á»ƒ cháº¡y RAG bot"""

    # Cáº¥u hÃ¬nh
    config = RAGConfig(
        vector_db_path="./vector_db",
        llm_provider="openai",  # Thá»­ OpenAI trÆ°á»›c
        llm_model="gpt-3.5-turbo",
        openai_api_key="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",
        # Temporary direct key
        temperature=0.1,
        retrieval_k=5,
        score_threshold=0.3  # Giáº£m threshold Ä‘á»ƒ dá»… tÃ¬m tÃ i liá»‡u hÆ¡n
    )

    try:
        # Khá»Ÿi táº¡o bot
        bot = VietnameseLegalRAGBot(config)
        bot.initialize()

        # Cháº¡y giao diá»‡n chat
        bot.chat_interface()

    except Exception as e:
        print(f"âŒ Lá»—i khá»Ÿi táº¡o: {e}")
        print("\nğŸ’¡ Kiá»ƒm tra láº¡i:")
        print("   â€¢ Vector database Ä‘Ã£ Ä‘Æ°á»£c táº¡o chÆ°a?")
        print("   â€¢ Ollama Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t vÃ  cháº¡y chÆ°a? (ollama serve)")
        print("   â€¢ Model llama2 Ä‘Ã£ Ä‘Æ°á»£c táº£i chÆ°a? (ollama pull llama2)")
        print("   â€¢ CÃ¡c thÆ° viá»‡n cáº§n thiáº¿t Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t chÆ°a?")


if __name__ == "__main__":
    main()